{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weave_example_demo.llm_types.prompts import PromptTemplate\n",
    "from weave_example_demo.llm_types.models.generic_model import GenericLLMModel\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weave.init('visa-bioasq-rag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Is Hirschsprung disease a mendelian or a multifactorial disorder?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_2_query_system_prompt = \"\"\"\n",
    "### Instruction ###\n",
    "You are an expert biomedical researcher tasked with converting biomedical questions into optimized semantic search queries. Your goal is to generate queries that will retrieve the most relevant documents from the BioASQ dataset to answer the given question.\n",
    "\n",
    "### Process ###\n",
    "Follow these steps to create the semantic search query:\n",
    "1. Carefully analyze the biomedical question to identify the most important keywords, concepts, and entities\n",
    "2. Construct a search query using those keywords, aiming to retrieve all potentially relevant documents\n",
    "3. Optimize the query by incorporating synonyms, related terms, and expanding acronyms if applicable\n",
    "4. Double check that the query captures the core intent of the question and will match pertinent documents\n",
    "5. Provide only the final semantic search query in your response, without any additional commentary\n",
    "\n",
    "### Context ###\n",
    "The BioASQ dataset consists of biomedical questions along with relevant documents. Your semantic search queries will be used to find the most relevant documents from this dataset to answer each question. The ideal answers have been removed, so your query should focus solely on the question text.\n",
    "\n",
    "### Examples ###\n",
    "Question: Is Hirschsprung disease a mendelian or a multifactorial disorder?\n",
    "Semantic Search Query: Hirschsprung disease AND (mendelian OR multifactorial OR complex) AND (inheritance OR genetics OR genes)\n",
    "\n",
    "Question: List signaling molecules (ligands) that interact with the receptor EGFR?  \n",
    "Semantic Search Query: EGFR AND (ligands OR \"signaling molecules\") AND (EGF OR BTC OR EPR OR HB-EGF OR TGF-Î± OR AREG OR EPG)\n",
    "\n",
    "Question: Is the protein Papilin secreted?\n",
    "Semantic Search Query: Papilin AND (secreted OR extracellular OR \"secretory pathway\")\n",
    "\n",
    "### Evaluation ###\n",
    "Your performance will be evaluated on:  \n",
    "- Inclusion of the most salient keywords, concepts and entities from the biomedical question\n",
    "- Appropriate use of synonyms and related terms to improve retrieval\n",
    "- Ability of the query to capture the full scope and intent of the question\n",
    "- Overall likelihood of the query retrieving documents that can answer the question\n",
    "- Adherence to the response format instructions\n",
    "\n",
    "You MUST provide a well-constructed query that fulfills the given criteria. You will be penalized for queries that are too narrow, off-topic, or poorly formulated.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_2_query_human_prompt = \"\"\"\n",
    "### Human Prompt ###\n",
    "Biomedical Question: \"{question}\"\n",
    "\n",
    "Semantic Search Query:\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_2_query_model = GenericLLMModel(\n",
    "    system_prompt=question_2_query_system_prompt,\n",
    "    human_prompt=question_2_query_human_prompt\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_query = question_2_query_model.predict(human_prompt_args={\"question\": question})['answer']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = weave.ref('VectorStore:latest').get()\n",
    "embedding_model = weave.ref('SentenceTransformersModel:latest').get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.set_embedding_model(embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.embeddings_matrix = np.array(\n",
    "            [doc_emb[\"embedding\"] for doc_emb in vector_store.article_embeddings]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_context = vector_store.get_most_relevant_documents(query=transformed_query, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_relevance_system_prompt = \"\"\"\n",
    "### Instruction ###\n",
    "You are an expert medical researcher librarian. Your task is to determine whether articles from the BioASQ dataset may be relevant to questions from clinicians based on the articles' abstracts. You MUST provide a yes or no answer. You will be penalized for answers that are not a clear yes or no.\n",
    "\n",
    "### Process ###\n",
    "1. Carefully read the provided clinical question. \n",
    "2. Analyze the given article abstract in the context of the question.\n",
    "3. Determine if the abstract contains information potentially relevant to answering the question. \n",
    "4. Provide a definitive yes or no answer. Do not hedge or equivocate.\n",
    "\n",
    "### Evaluation ###\n",
    "Your performance will be evaluated on:\n",
    "- Ability to identify abstracts with information relevant to the clinical question\n",
    "- Providing a clear, unambiguous yes or no answer \n",
    "- Avoiding reliance on stereotypes or biases in your determination\n",
    "- Adherence to the required answer format\n",
    "\n",
    "You MUST provide a yes or no answer. Any other response will be penalized.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_relevance_human_prompt = \"\"\"\n",
    "### Question ###\n",
    "Clinical question: \"{question}\"\n",
    "\n",
    "### Abstract ###\n",
    "{article_text}\n",
    "\n",
    "### Answer ###\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_relevance_model = GenericLLMModel(\n",
    "    system_prompt=article_relevance_system_prompt,\n",
    "    human_prompt=article_relevance_human_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in _context:\n",
    "    doc[\"relevance\"] = article_relevance_model.predict(human_prompt_args={\"question\": question, \"article_text\": doc[\"document\"][\"passage\"]})['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_context = [doc for doc in _context if doc[\"relevance\"].lower() == \"yes\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(relevant_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_context[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Add reranking using BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarization_system_prompt = \"\"\"\n",
    "### Instruction ###\n",
    "You are an expert medical researcher tasked with summarizing relevant excerpts from biomedical literature to provide background information necessary to answer clinicians' questions. Your summary should be concise yet informative, capturing the key points from the provided context.\n",
    "\n",
    "### Process ###\n",
    "1. Carefully read the provided clinical question to understand the information needed.\n",
    "2. Analyze the given context, which includes excerpts from biomedical literature along with relevance scores.\n",
    "3. Identify the most pertinent information from the context in relation to the question.\n",
    "4. Summarize the key points from the relevant excerpts, considering their relevance scores.\n",
    "5. Synthesize the individual summaries into a coherent overview addressing the question.\n",
    "6. If the context is not sufficient to answer the question, indicate that more information is needed.\n",
    "\n",
    "### Format ###\n",
    "Question: <question>\n",
    "Summary: <summary_of_relevant_information>\n",
    "Relevant Excerpts: <excerpts_in_order_of_relevance>\n",
    "\n",
    "### Evaluation ###\n",
    "Your performance will be evaluated on:\n",
    "- Ability to identify and summarize relevant information from the provided context\n",
    "- Synthesis of individual excerpt summaries into a coherent overview\n",
    "- Consideration of excerpt relevance scores in the final summary\n",
    "- Clarity and conciseness of the summary\n",
    "- Adherence to the specified response format\n",
    "\n",
    "You MUST provide a summary that directly addresses the given question using the most relevant excerpts from the context. If the provided context is insufficient to answer the question, state \"Insufficient information to answer the question.\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarization_human_prompt = \"\"\"\n",
    "### Question ###\n",
    "{question}\n",
    "\n",
    "### Context ###\n",
    "{context_str}\n",
    "\n",
    "### Summary ###\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarization_model = GenericLLMModel(\n",
    "    system_prompt=summarization_system_prompt,\n",
    "    human_prompt=summarization_human_prompt\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_str = \"\\n\\n\".join([f\"{doc['document']['passage']} (Score: {doc['score']})\" for doc in relevant_context])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = summarization_model.predict(human_prompt_args={\"question\": question, \"context_str\": context_str})['answer']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthesis_system_prompt = \"\"\"\n",
    "### Instruction ###\n",
    "You are an expert medical assistant. Your task is to provide accurate, concise answers to medical questions based on summaries of relevant biomedical literature. You MUST ensure responses are clear, informative, unbiased, and avoid stereotypes. Answer in a natural, human-like manner. You will be penalized for answers that are unclear, inaccurate, biased, or overly verbose.\n",
    "\n",
    "### Process ###\n",
    "1. Carefully analyze the provided question to understand the key information needed. \n",
    "2. Review the summary of relevant excerpts from biomedical literature.\n",
    "3. Identify the most pertinent information in the summary for answering the question.\n",
    "4. Synthesize the key points into a coherent, concise answer.\n",
    "5. If the summary lacks sufficient information to conclusively answer the question, state \"There is insufficient information provided to conclusively answer the question.\"\n",
    "\n",
    "### Format ###\n",
    "Question: <question>\n",
    "Answer: <final_answer_based_on_summary>\n",
    "\n",
    "### Example ###\n",
    "Question: Is Hirschsprung disease a mendelian or a multifactorial disorder?\n",
    "\n",
    "Summary: Hirschsprung disease, particularly in the context of Mowat-Wilson syndrome (MWS) associated with ZFHX1B mutations or deletions, shows variations in enteric neural plexus abnormalities. The pathologies in MWS are attributed to variations in ZFHX1B abnormalities and epigenetic factors.\n",
    "\n",
    "Relevant Excerpts: \n",
    "- Patients with ZFHX1B mutations or deletions develop multiple congenital anomalies including Hirschsprung disease, known as Mowat-Wilson syndrome (MWS). (Score: 0.6024968654169915)\n",
    "\n",
    "Answer: Based on the summary, Hirschsprung disease in Mowat-Wilson syndrome appears to have both genetic and multifactorial components. Variations in ZFHX1B abnormalities suggest a genetic basis, while the role of epigenetic factors points to a multifactorial etiology. However, the provided information is limited in conclusively determining if Hirschsprung disease more broadly is purely Mendelian or multifactorial.\n",
    "\n",
    "### Evaluation ###\n",
    "Your performance will be evaluated on:\n",
    "- Accuracy and relevance of the answer based on the provided summary\n",
    "- Clarity and conciseness of the response \n",
    "- Ability to identify when the summary is insufficient to conclusively answer the question\n",
    "- Avoidance of bias and stereotyping\n",
    "- Adherence to the specified format\n",
    "\n",
    "You MUST provide an answer that directly addresses the question using only the information in the summary. If the summary is insufficient, state that conclusively answering is not possible. Produce the answer in a clear, natural style.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthesis_human_prompt = \"\"\"\n",
    "### Question ###\n",
    "{question}\n",
    "\n",
    "### Summary ###\n",
    "{summary}\n",
    "\n",
    "### Answer ###\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthesis_model = GenericLLMModel(\n",
    "    system_prompt=synthesis_system_prompt,\n",
    "    human_prompt=synthesis_human_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthesis_model.predict(human_prompt_args={\"question\": question, \"summary\": summary})['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weave_example_demo.llm_types.rag.rag import RAGModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BioASQAdvancedRAGModel(RAGModel):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "    \n",
    "    @weave.op()\n",
    "    def score_context(self, _context) -> str:\n",
    "        for doc in _context:\n",
    "            doc[\"relevance\"] = article_relevance_model.predict(human_prompt_args={\"question\": question, \"article_text\": doc[\"document\"][\"passage\"]})['answer']\n",
    "        \n",
    "\n",
    "    @weave.op()\n",
    "    def predict(self, question: str, n_documents: int = 5) -> str:\n",
    "        self.set_vector_store(self.vector_store)\n",
    "        transformed_query = question_2_query_model.predict(human_prompt_args={\"question\": question})['answer']\n",
    "        _context = self.vector_store.get_most_relevant_documents(query=transformed_query, n=n_documents)\n",
    "        self.score_context(_context)\n",
    "        relevant_context = [doc for doc in _context if doc[\"relevance\"].lower() == \"yes\"]\n",
    "        # If no relevant context, use the most relevant document\n",
    "        # this is probably not the best but good for demonstrative\n",
    "        # purposes\n",
    "        if len(relevant_context) == 0:\n",
    "            relevant_context = [_context[0]]\n",
    "        context_str = \"\\n\\n\".join([f\"{doc['document']['passage']} (Score: {doc['score']})\" for doc in relevant_context])\n",
    "        summary = summarization_model.predict(human_prompt_args={\"question\": question, \"context_str\": context_str})['answer']\n",
    "        answer = synthesis_model.predict(human_prompt_args={\"question\": question, \"summary\": summary})['answer']\n",
    "        return {\"answer\": answer, \"context\": [doc[\"document\"][\"passage\"] for doc in relevant_context], \"all_context\": _context}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_model = BioASQAdvancedRAGModel(vector_store=vector_store)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_model.predict(question=question, n_documents=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qap = weave.ref('QuestionAnswerPairsTrainFiltered:latest').get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weave_example_demo.scorers.llm_guard_scorer import LLMGuardScorer\n",
    "from weave_example_demo.scorers.tonic_validate_scorer import TonicValidateScorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorers = [\n",
    "    TonicValidateScorer(\n",
    "        metrics=[\n",
    "            \"AnswerSimilarityMetric\",\n",
    "            \"AugmentationPrecisionMetric\",\n",
    "            \"AnswerConsistencyMetric\",\n",
    "        ]\n",
    "    ),\n",
    "    LLMGuardScorer(\n",
    "        metrics=[\"NoRefusal\", \"Relevance\", \"Sensitive\"]),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_qap = qap.rows[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_qap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = weave.Evaluation(dataset=sub_qap, scorers=scorers)\n",
    "await evaluation.evaluate(rag_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
